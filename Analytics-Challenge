# https://www.mindsumo.com/contests/campus-analytics-challenge-2021

# Import libraries
import pandas as pd
import numpy as np
import datetime
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
import holidays
import statsmodels.api as sm
from sklearn.metrics import classification_report

# Create the training datasets
mldf = pd.read_excel('https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/479/b765dc3d8076-trainset+%281%29.xlsx', parse_dates = ['PWD_UPDT_TS', 'PH_NUM_UPDT_TS', 'CUST_SINCE_DT', 'TRAN_TS', 'TRAN_DT', 'ACTVY_DT'])

# Convert the dependent variable to a binary value
mldf.FRAUD_NONFRAUD = np.where(mldf.FRAUD_NONFRAUD == 'Fraud', 1, 0)

# Create a value for the dependent variable so that it is easier to identify
dependent = 'FRAUD_NONFRAUD'

# Add a weekday field
mldf['Weekday'] = pd.DatetimeIndex(mldf.TRAN_TS).weekday
mldf['Weekday'] = mldf.Weekday.replace({0 : 'Monday', 1 : 'Tuesday', 2 : 'Wednesday', 3 : 'Thursday', 4 : 'Friday', 5 : 'Saturday', 6 : 'Sunday'})
mldf['Month'] = pd.DatetimeIndex(mldf.TRAN_TS).month
mldf['Month'] = mldf['Month'].replace({1 : 'Jan', 2 : 'Feb', 3 : 'Mar', 4 : 'Apr', 5 : 'May', 6 : 'Jun', 7 : 'Jul', 8 : 'Aug', 9 : 'Sep', 10 : 'Oct', 11  : 'Nov', 12 : 'Dec'})
mldf['Year'] = pd.DatetimeIndex(mldf.TRAN_TS).year

# Add a holiday field
for i in range(len(mldf)):
    mldf.loc[i, 'Holiday'] = np.where(mldf.loc[i, 'TRAN_TS'] in holidays.UnitedStates(), 1, 0)

# Create a field showing when the transaction exceeded the account balance
mldf['OVERDRAW'] = np.where(mldf.TRAN_AMT > mldf.ACCT_PRE_TRAN_AVAIL_BAL, 1, 0)

# Create a field denoting what percent of the total balance was transacted - .01 is added to allow this to work with accounts that have a zero balance
mldf['PERCENT_BAL'] = mldf.TRAN_AMT/(mldf.ACCT_PRE_TRAN_AVAIL_BAL + .01)

# Create a zero balance flag
mldf['ZERO_BAL'] = np.where(mldf.ACCT_PRE_TRAN_AVAIL_BAL == 0, 1, 0)

# Fill password and phone update fields with the CUST_SINCE_DT value
for i in ['PWD_UPDT_TS', 'PH_NUM_UPDT_TS']:
    mldf[i] = mldf[i].str.split(' ').str[0]
    
# Create a field denoting false dates
mldf['FALSE_DATE'] = np.where((mldf['PWD_UPDT_TS'] == '6/31/2020') | (mldf['PWD_UPDT_TS'] == '6/31/2018') | (mldf['PH_NUM_UPDT_TS'] == '3/0/2019'), 1, 0)

# Fill the false date with a null value since pandas cannot handle it
mldf['PWD_UPDT_TS'] = np.where((mldf['PWD_UPDT_TS'] == '6/31/2020') | (mldf['PWD_UPDT_TS'] == '6/31/2018') | (mldf['PWD_UPDT_TS'] == '3/0/2019'), np.nan, mldf['PWD_UPDT_TS'])
mldf['PH_NUM_UPDT_TS'] = np.where((mldf['PH_NUM_UPDT_TS'] == '6/31/2020') | (mldf['PH_NUM_UPDT_TS'] == '6/31/2018') | (mldf['PH_NUM_UPDT_TS'] == '3/0/2019'), np.nan, mldf['PWD_UPDT_TS'])
    
# Calculate time differences between password/phone updates, onboarding dates, and transaction dates
mldf['ONBOARD_TO_TRAN'] = (pd.to_datetime(mldf.CUST_SINCE_DT) - pd.to_datetime(mldf.TRAN_DT)).dt.days
mldf['PWD_PHONE_DIFF'] = (pd.to_datetime(mldf.PWD_UPDT_TS) - pd.to_datetime(mldf.PH_NUM_UPDT_TS)).dt.days
mldf['PWD_TRAN_DIFF'] = (pd.to_datetime(mldf.PWD_UPDT_TS) - pd.to_datetime(mldf.TRAN_DT)).dt.days
mldf['PHONE_TRAN_DIFF'] = (pd.to_datetime(mldf.PH_NUM_UPDT_TS) - pd.to_datetime(mldf.TRAN_DT)).dt.days

# Fill the null values from the calculations above with the mean of those columns
for i in ['PWD_PHONE_DIFF', 'PWD_TRAN_DIFF', 'PHONE_TRAN_DIFF']:
    mldf[i] = mldf[i].fillna(mldf[i].mean())
    
# Calculate how long the have been a customer for
mldf['CUST_YEARS'] = datetime.date.today()
mldf['CUST_YEARS'] = round((pd.to_datetime(mldf.CUST_YEARS) - pd.to_datetime(mldf.CUST_SINCE_DT))/datetime.timedelta(days = 365))

# Convert categorical date values to dummy columns
for i in ['Year', 'Month', 'Weekday', 'RGN_NAME', 'STATE_PRVNC_TXT', 'ALERT_TRGR_CD', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT', 'TRAN_TYPE_CD']:
    try:
        dummies = pd.get_dummies(mldf[i])
        mldf = mldf.merge(dummies, left_index = True, right_index = True)
        mldf.drop(i, inplace = True, axis = 1)
    except: pass

######################################################################################################################################################################
''' LOOP THROUGH THE REMAINING FACTORS AND DROP THE ONES WITH P VALUES < .05 (this is a stepwise regression) '''    

# Calculate the correlations for each variable
correlations = mldf.corr()[dependent]
correlations = pd.DataFrame(abs(correlations)).sort_values(dependent, ascending = False)
    
# Drop the 'advance' booking amounts that are more than +/- two weeks from the depart date
factors = list(correlations.index[1:])

# Create a variables field
variables = factors + [dependent]

# Set p equal to 1 initially, then reset it to the highest p-value of each run. End the loop when the highest p-value is below .05
p=1

# Run a stepwise, throwing out variables without statistical significance
while p > .001: 
    
    # Create a dataframe to work from
    df = mldf[variables]
    df = df.loc[:,~df.columns.duplicated()]
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace = True)
    
    # Specify variables
    y = df[dependent]
    x = df.drop([dependent], axis = 1)
    cols = x.columns   
    
    # Return the stats
    results = sm.OLS(y, x).fit()
    print(results.summary())
          
    # Create a dataframe of results
    pv = pd.DataFrame(results.pvalues).sort_values(0, ascending = False).reset_index(drop = False).rename(columns = {'index' : 'Variable', 0 : 'p'})
    
    # Set p equal to the highest p-value
    p = pv.loc[0,'p']
    
    # If the highest p-value exceeds .05, remove it from the model
    if p > .001:
        variables.remove(pv.loc[0, 'Variable'])
    else:
        print(results.summary())
        print('')
 
f1 = 0
def Evaluate():
    
    # Subset the main dataset to the remaining variables
    df = mldf[variables]
    df = df.loc[:,~df.columns.duplicated()]
    
    # Specify factors
    y = df[dependent]
    x = df[variables].drop([dependent], axis = 1)
    cols = x.columns
    
    # Create a scaler variable and scale the independent variables
    scaler = StandardScaler()
    scaler.fit(x)
    x = scaler.transform(x)
    
    # Cross validate the model
    scores = cross_val_score(AdaBoostClassifier(n_estimators = 1000), x, y, cv = 10, n_jobs = 3, scoring = 'accuracy')
    global f1
    f1 = cross_val_score(AdaBoostClassifier(n_estimators = 1000), x, y, cv = 10, n_jobs = 3, scoring = 'f1')
    
    # Evaluate the model
    accuracy = round(scores.mean(),5)
    f1 = round(f1.mean(),5)
    print('Updated accuracy: {}\nUpdated f1: {}'.format(accuracy, f1))

Evaluate()
print('F1 IS {}'.format(str(f1)))

######################################################################################################################################################################
''' LOOP THROUGH EACH REMAINING FACTOR AND DROP THE ONES THAT DO NOT REDUCE THE ERROR '''

# Create a dataframe of just the remaining factors
df = mldf[variables]

# Calculate the correlations for each variable - we are doing this so that we can work our way down from most to least correlated
correlations = df.corr()[dependent]
correlations = pd.DataFrame(abs(correlations)).sort_values(dependent, ascending = True)
correlations = correlations[correlations[dependent] != 1]

# Drop the 'advance' booking amounts that are more than +/- two weeks from the depart date
variables = list(correlations.index)
variables.append(dependent)

# Reset the factors to a list containing just the dependent variable, then loop through the variables from the stepwise and only keep ones that reduce RMSE
factors = variables

for i in variables:
    
    try:
        # Add the next independent variable to the list
        factors.remove(i)
        
        # Create a dataframe variable to work with
        df = mldf[factors + [dependent]]
        df = df.dropna()
        df = df.loc[:,~df.columns.duplicated()]
        
        # Specify factors
        y = df[dependent]
        x = df[factors].drop([dependent], axis = 1)
        cols = x.columns

        # Create a scaler variable and scale the independent variables
        scaler = StandardScaler()
        scaler.fit(x)
        x = scaler.transform(x)

        # Cross validate the model
        scores = cross_val_score(AdaBoostClassifier(n_estimators = 1000), x, y, cv = 10, n_jobs = 3, scoring = 'f1')
    
        # Evaluate the model
        acc = round(scores.mean(),5)
        
        if acc >= f1:
            print('Dropped {}. F1 is now {} with {} factors'.format(str(i), str(acc), str(len(factors))))
            f1 = acc
        else:
            factors.append(i)
            print('Kept ' + str(i))
    except:
        pass
  
factors.append(dependent)
variables = factors   
Evaluate()
backup = factors

######################################################################################################################################################################
''' LOOP THROUGH EACH REMAINING FACTOR, STARTING WITH NOTHING, AND ONLY ADD THE ONES THAT INCREASE THE F1 '''

# Create a dataframe of just the remaining factors
df = mldf[factors]

# Calculate the correlations for each variable - we are doing this so that we can work our way down from most to least correlated
correlations = df.corr()[dependent]
correlations = pd.DataFrame(abs(correlations)).sort_values(dependent, ascending = False)
    
# Drop the 'advance' booking amounts that are more than +/- two weeks from the depart date
factors = list(correlations.index[1:])

# Create a variables field
variables = factors #+ [dependent]

factors = [dependent]
accuracy = 0

for i in variables:
    
    try:
        # Add the next independent variable to the list
        factors.append(i)
        
        # Create a dataframe variable to work with
        df = mldf[factors]
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df = df.loc[:,~df.columns.duplicated()]
        df = df.dropna()
        
        # Specify factors
        y = df[dependent]
        x = df[factors].drop([dependent], axis = 1)
        cols = x.columns

        # Create a scaler variable and scale the independent variables
        scaler = StandardScaler()
        scaler.fit(x)
        x = scaler.transform(x)
        
        # Cross validate the model
        scores = cross_val_score(AdaBoostClassifier(n_estimators = 1000), x, y, cv = 10, n_jobs = 3, scoring = 'f1')
    
        # Evaluate the model
        acc = round(scores.mean(),5)
        
        if acc <= accuracy:
            factors.remove(i)
            print('Dropped ' + str(i))
        else:
            accuracy = acc
            print('F1 for {}: {} \n'.format(i,acc))
    except:
        pass

variables = factors    
Evaluate()

''' Updated accuracy: 
    Updated f1:  '''

# Create the training data
x = mldf[variables]
x.drop(dependent, axis = 1, inplace = True)
y = mldf[dependent]

# Split the values into training and test
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = .7)

# Fit the model to the training data
model = AdaBoostClassifier(n_estimators = 1000)
model.fit(x_train,y_train)

pred = model.predict(x_test)
print(classification_report(y_test, pred))

#================================================================================================================================================#
''' PREDICT VALUES FOR THE TEST DATA '''

# Load the test data
test = pd.read_excel('https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/479/b765dc3d8076-testset_for_participants.xlsx', parse_dates = ['PWD_UPDT_TS', 'PH_NUM_UPDT_TS', 'CUST_SINCE_DT', 'TRAN_TS', 'TRAN_DT', 'ACTVY_DT'])

# Add a weekday field
test['Weekday'] = pd.DatetimeIndex(test.TRAN_TS).weekday
test['Weekday'] = test.Weekday.replace({0 : 'Monday', 1 : 'Tuesday', 2 : 'Wednesday', 3 : 'Thursday', 4 : 'Friday', 5 : 'Saturday', 6 : 'Sunday'})
test['Month'] = pd.DatetimeIndex(test.TRAN_TS).month
test['Month'] = test['Month'].replace({1 : 'Jan', 2 : 'Feb', 3 : 'Mar', 4 : 'Apr', 5 : 'May', 6 : 'Jun', 7 : 'Jul', 8 : 'Aug', 9 : 'Sep', 10 : 'Oct', 11  : 'Nov', 12 : 'Dec'})
test['Year'] = pd.DatetimeIndex(test.TRAN_TS).year

# Add a holiday field
for i in range(len(test)):
    test.loc[i, 'Holiday'] = np.where(test.loc[i, 'TRAN_TS'] in holidays.UnitedStates(), 1, 0)

# Create a field showing when the transaction exceeded the account balance
test['OVERDRAW'] = np.where(test.TRAN_AMT > test.ACCT_PRE_TRAN_AVAIL_BAL, 1, 0)

# Create a field denoting what percent of the total balance was transacted - .01 is added to allow this to work with accounts that have a zero balance
test['PERCENT_BAL'] = test.TRAN_AMT/(test.ACCT_PRE_TRAN_AVAIL_BAL + .01)

# Create a zero balance flag
test['ZERO_BAL'] = np.where(test.ACCT_PRE_TRAN_AVAIL_BAL == 0, 1, 0)

# Fill password and phone update fields with the CUST_SINCE_DT value
for i in ['PWD_UPDT_TS', 'PH_NUM_UPDT_TS']:
    test[i] = test[i].str.split(' ').str[0]
    
# Create a field denoting false dates
test['FALSE_DATE'] = np.where((test['PWD_UPDT_TS'] == '6/31/2020') | (test['PWD_UPDT_TS'] == '6/31/2018') | (test['PH_NUM_UPDT_TS'] == '3/0/2019') | (test['PWD_UPDT_TS'] == '6/31/2021'), 1, 0)

# Fill the false date with a null value since pandas cannot handle it
test['PWD_UPDT_TS'] = np.where((test['PWD_UPDT_TS'] == '6/31/2020') | (test['PWD_UPDT_TS'] == '6/31/2018') | (test['PWD_UPDT_TS'] == '3/0/2019') | (test['PWD_UPDT_TS'] == '6/31/2021'), np.nan, test['PWD_UPDT_TS'])
test['PH_NUM_UPDT_TS'] = np.where((test['PH_NUM_UPDT_TS'] == '6/31/2020') | (test['PH_NUM_UPDT_TS'] == '6/31/2018') | (test['PH_NUM_UPDT_TS'] == '3/0/2019'), np.nan, test['PWD_UPDT_TS'])
    
# Calculate time differences between password/phone updates, onboarding dates, and transaction dates
test['ONBOARD_TO_TRAN'] = (pd.to_datetime(test.CUST_SINCE_DT) - pd.to_datetime(test.TRAN_DT)).dt.days
test['PWD_PHONE_DIFF'] = (pd.to_datetime(test.PWD_UPDT_TS) - pd.to_datetime(test.PH_NUM_UPDT_TS)).dt.days
test['PWD_TRAN_DIFF'] = (pd.to_datetime(test.PWD_UPDT_TS) - pd.to_datetime(test.TRAN_DT)).dt.days
test['PHONE_TRAN_DIFF'] = (pd.to_datetime(test.PH_NUM_UPDT_TS) - pd.to_datetime(test.TRAN_DT)).dt.days

# Fill the null values from the calculations above with the mean of those columns
for i in ['PWD_PHONE_DIFF', 'PWD_TRAN_DIFF', 'PHONE_TRAN_DIFF']:
    test[i] = test[i].fillna(test[i].mean())
    
# Calculate how long the have been a customer for
test['CUST_YEARS'] = datetime.date.today()
test['CUST_YEARS'] = round((pd.to_datetime(test.CUST_YEARS) - pd.to_datetime(test.CUST_SINCE_DT))/datetime.timedelta(days = 365))

# Convert categorical date values to dummy columns
for i in ['Year', 'Month', 'Weekday', 'RGN_NAME', 'STATE_PRVNC_TXT', 'ALERT_TRGR_CD', 'DVC_TYPE_TXT', 'AUTHC_PRIM_TYPE_CD', 'AUTHC_SCNDRY_STAT_TXT', 'TRAN_TYPE_CD']:
    try:
        dummies = pd.get_dummies(test[i])
        test = test.merge(dummies, left_index = True, right_index = True)
        test.drop(i, inplace = True, axis = 1)
    except: pass

# Create the training data
x = mldf[variables]
try:
    x.drop(dependent, axis = 1, inplace = True)
except: pass
y = mldf[dependent]

# Fit the model to the training data
model = AdaBoostClassifier(n_estimators = 1000)
model.fit(x,y)

# Predict the values in the test data
try:
    variables.remove(dependent)
except:
    pass
test = test[variables + ['dataset_id']].set_index('dataset_id')
test['FRAUD_NONFRAUD'] = model.predict(test)

# Subset the test dataset to the relevant fields
test = test[['FRAUD_NONFRAUD']].reset_index(drop = False).rename(columns = {'index' : 'dataset_id'})
test.FRAUD_NONFRAUD.replace({0 : 'Non-Fraud', 1 : 'Fraud'}, inplace = True)

# Export the results
test.to_csv('C:\\Users\\conno\\OneDrive\\Desktop\\Challenge\\Connor Minney Test Results.csv', index = False)
